# Latent Dirichlet Allocation

- 각 문서의 주제 분포와 각 주제 내의 단어 분포를 추정

- 문서1 : 규찬이는 딥페이크와 객체 탐지를 공부해요
- 문서2 : 민화는 재밌는 자연어처리가 좋아요
- 문서3 : 형훈이는 스릴있고 재밌는 자연어처리에 객체 탐지까지 공부해요

- 위의 문서에서 2개의 주제를 찾는다고 가정
- 전처리 과정은 진행했다고 가정

## <각 문서의 주제 분포>
- 문서 1 : CV 100%
- 문서 2 : NLP 100%
- 문서 3 : CV 40%, NLP 60%

## <각 주제 단어 분포>
- CV : 딥페이크 20%, 객체 탐지 40%, 공부해요 40%
- NLP : 재밌는 33%, 자연어처리 33%, 좋아요 16%, 스릴있는 16%
# LDA 가정

- BoW DTM 또는 TF-IDF 행렬를 입력으로
- 순서는 상관하지 않음

1. 문서에 사용할 단어의 개수 N을 지정
2. 문서에 사용할 주제의 혼합을 확률 분포에 기반하여 결정
3. 문서에 사용할 각 단어를 아래와 같이 정함
3-1). 60% 확률로 자연어 처리에 대한 주제를 선택하고, 40% 확률로 CV 주제를 선택할 수 있음
3-2). 선택한 주제 T에서 단어의 출현 확률 분포에 기반해 문서에 사용할 단어를 고름
- ex) 자연어치리 주제를 선택하였다면, 33% 확률로 자연어처리 라는 단어를 선택할 수 있음

# LDA 수행

## 1. 사용자는 주제의 개수를 알려줌
- k개의 주제 수를 입력받으면, k개의 주제가 M개의 전체 문서에 걸쳐 분포되어 있다고 가정

## 2. 모든 단어를 k개 중 하나의 주제에 할당
- 모든 문서의 모든 단어에 대해서 k개 중 하나의 주제를 무작위로 할당
- 이 작업이 끝나면 모든 문서는 주제를 가지며, 주제는 단어 분포를 가지는 상태

## 3. 어떤 문서의 각 단어 w는 자신이 잘못된 주제에 할당되어 있다고 가정(다른 단어들은 올바르게 되었다고 가정)
- P(주제 t | 문서 d) : 문서 d의 단어들 중 주제 t에 해당하는 단어의 비율
- P(단어 w | 주제 t) : 각 주제 t에서 해당 단어 w의 분포
- 이 과정을 모든 단어에 대하여 반복

<img width="279" alt="캡처" src="https://user-images.githubusercontent.com/80622859/201518693-d6c11792-403d-4826-8ce7-5645ba7ffacb.PNG">

- 초기 상태(무작위로 주제를 배정받은 후의 상태)

### 1. 문서 1의 단어들이 어떤 주제에 해당하는지
- 문서 1에서 주제 A와 주제 B의 개수는 각각 2개, 2개
- apple은 50% 확률로 A 또는 B에 배정받을 수 있음

### 2. 단어 apple이 전체 문서에서 어떤 주제에 할당되어져 있는지
- 100% 확률로 B
- apple은 B에 할당될 가능성이 높음

- 1번과 2번에서 나온 값들을 곱해서 최종적인 확률 계산
- 

# Topic 수 정하기

## Perplexity(혼란도)
- 특정 확률 모델이 실제로 관측되는 값을 얼마나 잘 예측하는지
- 값이 작을수록 토픽 모델이 문서를 잘 반영한다고 볼 수 있음

<img width="448" alt="캡처" src="https://user-images.githubusercontent.com/80622859/202167194-d40a4d1f-20d6-4087-b70d-749e0eb6b426.PNG">

- P : 문장에 대한 Generation probability 의미
- 조건부 확률의 연쇄법칙을 활용하여 나타냄
- 역수의 기하평균
- 문장 생성 확률의 역수를 단어의 수(N)로 정규화
- 문장의 발생 확률이 높을수록 문장에 대한 혼란도는 낮아짐
- 문장 발생확률의 역수는 발생할 수 있는 가짓수

- 토픽 모델링 기법이 얼마나 빠르게 수렴하는 지 확인할 때
- 확률 모델이 다른 모델에 비해 얼마나 개선되었는지 평가할 때
- 동일 모델 내 파라미터에 따른 성능 평가할 때

## Coherence(응집도)
- 주제가 얼마나 의미론적으로 일관성이 있는지
- 높을수록 의미론적 일관성이 높음
- 해당 모델이 얼마나 실제로 의미있는 결과를 내는지 확인하기 위해

- 각각의 주제에서 상위 N개의 단어를 뽑음
- 그 상위 N개의 유사도를 계산
