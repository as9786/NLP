{"cells":[{"cell_type":"markdown","metadata":{"id":"G0NV324ZcL9L"},"source":["## Settings and run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rxg0y5MBudmd"},"outputs":[],"source":["# Name/Path of the initial model.\n","MODEL_NAME = \"stabilityai/stable-diffusion-2-base\"\n","\n","# Directory name to save model at.\n","\n","OUTPUT_DIR = \"musinsa/stable_diffusion_weights/modeloutput\"\n","\n","\n","OUTPUT_DIR = \"/content/drive/MyDrive/\" + OUTPUT_DIR\n","\n","print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n","\n","!mkdir -p $OUTPUT_DIR"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qn5ILIyDJIcX"},"source":["# Start Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5vDpCxId1aCm"},"outputs":[],"source":["# You can also add multiple concepts here. Try tweaking `--max_train_steps` accordingly.\n","\n","concepts_list = [\n","    {\n","        \"instance_prompt\":      \"a person wearing  Musinsa_office_style clothes\",\n","        \"class_prompt\":         \"a person wearing clothes\",\n","        \"instance_data_dir\":    \"/content/drive/MyDrive/musinsa/key_pic/office\",\n","        \"class_data_dir\":       \"/content/drive/MyDrive/musinsa/옷 코디\"\n","    },\n","    {\n","        \"instance_prompt\":      \"a person wearing Musinsa_comfortable_style clothes\",\n","        \"class_prompt\":         \"a person wearing clothes\",\n","        \"instance_data_dir\":    \"/content/drive/MyDrive/musinsa/key_pic/comfortable\",\n","        \"class_data_dir\":       \"/content/drive/MyDrive/musinsa/옷 코디\"\n","    }\n","]\n","\n","concepts_list = [\n","    {\n","        \"instance_prompt\":      \"a person wearing  Musinsa_tropical_style clothes\",\n","        \"class_prompt\":         \"a person wearing clothes\",\n","        \"instance_data_dir\":    \"/content/drive/MyDrive/musinsa/key_pic/트로피컬\",\n","        \"class_data_dir\":       \"/content/drive/MyDrive/musinsa/옷 코디\"\n","    },\n","    {\n","        \"instance_prompt\":      \"a person wearing Musinsa_christmas_style clothes\",\n","        \"class_prompt\":         \"a person wearing clothes\",\n","        \"instance_data_dir\":    \"/content/drive/MyDrive/musinsa/key_pic/크리스마스\",\n","        \"class_data_dir\":       \"/content/drive/MyDrive/musinsa/옷 코디\"\n","    },\n","    {\n","        \"instance_prompt\":      \"a person wearing clothes for Musinsa_picnic\",\n","        \"class_prompt\":         \"a person wearing clothes\",\n","        \"instance_data_dir\":    \"/content/drive/MyDrive/musinsa/key_pic/피크닉\",\n","        \"class_data_dir\":       \"/content/drive/MyDrive/musinsa/옷 코디\"\n","    },\n","    {\n","        \"instance_prompt\":      \"a person wearing clothes for Musinsa_party\",\n","        \"class_prompt\":         \"a person wearing clothes\",\n","        \"instance_data_dir\":    \"/content/drive/MyDrive/musinsa/key_pic/파티\",\n","        \"class_data_dir\":       \"/content/drive/MyDrive/musinsa/옷 코디\"\n","    },\n","]\n","\n","\n","# `class_data_dir` contains regularization images\n","import json\n","import os\n","for c in concepts_list:\n","    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n","\n","with open(\"drive/MyDrive/musinsa/tropical_picnic_christmas_party.json\", \"w\") as f:\n","    json.dump(concepts_list, f, indent=4)"]},{"cell_type":"markdown","metadata":{"id":"c6SVgT47s72V"},"source":["# Fine-tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jjcSXTp-u-Eg"},"outputs":[],"source":["!accelerate launch /content/drive/MyDrive/musinsa/train_dreambooth.py \\\n","  --pretrained_model_name_or_path=$MODEL_NAME \\\n","  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n","  --output_dir=$OUTPUT_DIR \\\n","  --revision=\"fp16\" \\\n","  --with_prior_preservation --prior_loss_weight=1.0 \\\n","  --seed=1337 \\\n","  --resolution=512 \\\n","  --train_batch_size=1 \\\n","  --train_text_encoder \\\n","  --mixed_precision=\"fp16\" \\\n","  --use_8bit_adam \\\n","  --gradient_accumulation_steps=1 \\\n","  --learning_rate=1e-6 \\\n","  --lr_scheduler=\"constant\" \\\n","  --lr_warmup_steps=0 \\\n","  --num_class_images=100 \\\n","  --sample_batch_size=4 \\\n","  --max_train_steps=3000 \\\n","  --save_interval=10000 \\\n","  --save_sample_prompt=\"a person wearing  Musinsa_tropical_style clothes for Musinsa_picnic\" \\\n","  --concepts_list=\"/content/drive/MyDrive/musinsa/tropical_picnic_christmas_party.json\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"89Az5NUxOWdy"},"outputs":[],"source":["from natsort import natsorted\n","from glob import glob\n","import os\n","WEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]\n","print(f\"[*] WEIGHTS_DIR={WEIGHTS_DIR}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QmF-dpxaUYw7"},"outputs":[],"source":["#Run to generate a grid of preview images from the last saved weights.\n","import os\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","\n","weights_folder = OUTPUT_DIR\n","folders = sorted([f for f in os.listdir(weights_folder) if f != \"0\"], key=lambda x: int(x))\n","\n","row = len(folders)\n","col = len(os.listdir(os.path.join(weights_folder, folders[0], \"samples\")))\n","scale = 4\n","fig, axes = plt.subplots(row, col, figsize=(col*scale, row*scale), gridspec_kw={'hspace': 0, 'wspace': 0})\n","\n","for i, folder in enumerate(folders):\n","    folder_path = os.path.join(weights_folder, folder)\n","    image_folder = os.path.join(folder_path, \"samples\")\n","    images = [f for f in os.listdir(image_folder)]\n","    for j, image in enumerate(images):\n","        if row == 1:\n","            currAxes = axes[j]\n","        else:\n","            currAxes = axes[i, j]\n","        if i == 0:\n","            currAxes.set_title(f\"Image {j}\")\n","        if j == 0:\n","            currAxes.text(-0.1, 0.5, folder, rotation=0, va='center', ha='center', transform=currAxes.transAxes)\n","        image_path = os.path.join(image_folder, image)\n","        img = mpimg.imread(image_path)\n","        currAxes.imshow(img, cmap='gray')\n","        currAxes.axis('off')\n","        \n","plt.tight_layout()\n","plt.savefig('grid.png', dpi=72)"]},{"cell_type":"markdown","metadata":{"id":"5V8wgU0HN-Kq"},"source":["## Convert weights to ckpt to use in web UIs like AUTOMATIC1111."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dcXzsUyG1aCy"},"outputs":[],"source":["# Run conversion.\n","ckpt_path = WEIGHTS_DIR + \"/model.ckpt\"\n","\n","half_arg = \"\"\n","# Whether to convert to fp16, takes half the space (2GB).\n","fp16 = True \n","if fp16:\n","    half_arg = \"--half\"\n","\n","!python3 /content/drive/MyDrive/musinsa/convert_diffusers_to_original_stable_diffusion.py --model_path $WEIGHTS_DIR  --checkpoint_path $ckpt_path $half_arg\n","print(f\"[*] Converted ckpt saved at {ckpt_path}\")"]},{"cell_type":"markdown","metadata":{"id":"ToNG4fd_dTbF"},"source":["## Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gW15FjffdTID"},"outputs":[],"source":["import torch\n","from torch import autocast\n","from diffusers import StableDiffusionPipeline, DDIMScheduler\n","from IPython.display import display\n","model_path =   '/content/drive/MyDrive/musinsa/stable_diffusion_weights/modeloutput/3000'          # If you want to use previously trained model saved in gdrive, replace this with the full path of model in gdrive\n","\n","scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n","pipe = StableDiffusionPipeline.from_pretrained(model_path, scheduler=scheduler, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n","\n","g_cuda = None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D3VHmWGZDKOs"},"outputs":[],"source":["unet = UNet2DConditionModel.from_pretrained(\n","        model_path,\n","        subfolder=\"unet\",\n","        revision=\"fp16\",\n","        torch_dtype=torch.float32\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oIzkltjpVO_f"},"outputs":[],"source":["# Can set random seed here for reproducibility.\n","g_cuda = torch.Generator(device='cuda')\n","seed = 715\n","g_cuda.manual_seed(seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K6xoHWSsbcS3","scrolled":false},"outputs":[],"source":["prompt = \"a person wearing Musinsa_tropical_style clothes\" \n","negative_prompt = \"blurry, bad fingers\" \n","num_samples = 4 \n","guidance_scale = 7.5\n","num_inference_steps = 50\n","height = 512 \n","width = 512 \n","\n","with autocast(\"cuda\"), torch.inference_mode():\n","    images = pipe(\n","        prompt,\n","        height=height,\n","        width=width,\n","        negative_prompt=negative_prompt,\n","        num_images_per_prompt=num_samples,\n","        num_inference_steps=num_inference_steps,\n","        guidance_scale=guidance_scale,\n","        generator=g_cuda\n","    ).images\n","\n","for i,img in enumerate(images):\n","    display(img)\n","    img.save(f'/content/drive/MyDrive/musinsa/create/{prompt}_{i}.jpg',\"JPEG\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WMCqQ5Tcdsm2"},"outputs":[],"source":["\n","import gradio as gr\n","\n","def inference(prompt, negative_prompt, num_samples, height=512, width=512, num_inference_steps=50, guidance_scale=7.5):\n","    with torch.autocast(\"cuda\"), torch.inference_mode():\n","        return pipe(\n","                prompt, height=int(height), width=int(width),\n","                negative_prompt=negative_prompt,\n","                num_images_per_prompt=int(num_samples),\n","                num_inference_steps=int(num_inference_steps), guidance_scale=guidance_scale,\n","                generator=g_cuda\n","            ).images\n","\n","with gr.Blocks() as demo:\n","    with gr.Row():\n","        with gr.Column():\n","            prompt = gr.Textbox(label=\"Prompt\", value=\"photo of zwx dog in a bucket\")\n","            negative_prompt = gr.Textbox(label=\"Negative Prompt\", value=\"\")\n","            run = gr.Button(value=\"Generate\")\n","            with gr.Row():\n","                num_samples = gr.Number(label=\"Number of Samples\", value=4)\n","                guidance_scale = gr.Number(label=\"Guidance Scale\", value=7.5)\n","            with gr.Row():\n","                height = gr.Number(label=\"Height\", value=512)\n","                width = gr.Number(label=\"Width\", value=512)\n","            num_inference_steps = gr.Slider(label=\"Steps\", value=50)\n","        with gr.Column():\n","            gallery = gr.Gallery()\n","\n","    run.click(inference, inputs=[prompt, negative_prompt, num_samples, height, width, num_inference_steps, guidance_scale], outputs=gallery)\n","\n","demo.launch(debug=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lJoOgLQHnC8L"},"outputs":[],"source":["import shutil\n","from glob import glob\n","import os\n","for f in glob(OUTPUT_DIR+os.sep+\"*\"):\n","    if f != WEIGHTS_DIR:\n","        shutil.rmtree(f)\n","        print(\"Deleted\", f)\n","for f in glob(WEIGHTS_DIR+\"/*\"):\n","    if not f.endswith(\".ckpt\") or not f.endswith(\".json\"):\n","        try:\n","            shutil.rmtree(f)\n","        except NotADirectoryError:\n","            continue\n","        print(\"Deleted\", f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jXgi8HM4c-DA"},"outputs":[],"source":["#@title Free runtime memory \n","exit()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EdCegr4-555i"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","private_outputs":true,"provenance":[{"file_id":"1K3FPvW-VfG5EqH0rIngVrHVxZH-RtTVi","timestamp":1671198017318},{"file_id":"1ehPO8D-jAKg5kG4IvuTLQUP-QoUAOIky","timestamp":1669372233485},{"file_id":"https://github.com/ShivamShrirao/diffusers/blob/main/examples/dreambooth/DreamBooth_Stable_Diffusion.ipynb","timestamp":1669355145756}]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"vscode":{"interpreter":{"hash":"e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"}}},"nbformat":4,"nbformat_minor":0}
