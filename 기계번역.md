# Machine Translation and Encoder-Decoder Models

- 현재 형태의 기계 번역은 매우 실용적인 작업에 초점을 맞춤
- 인간 번역가를 도움
- 통신에 도움
- 순환신경망 또는 transformer로 구현할 수 있는 구조인 seq2seq이라 불리는 encoder-decoder 구조의 신경망
- 입력 단어 또는 token sequence에서 개별 단어의 직접 매핑이 아닌 tag sequence로 매핑해야 함
- 언어의 단어가 숫자나 순서에서 소스 언어의 단어와 반드시 일치하지 않음. ex) 일본 -> 영어
- 문장의 요소들이 언어마다 매우 다른 위치에 있음
- 대소문자 유무
- seq2seq은 기계 번역만을 위한것이 아니라 두 시퀀스 간의 복잡한 매핑이 관련된 다른 많은 작업을 위한 최신 기술. ex)요약, 대화, 의미 구문 분석 등

## 10.1 Language Divergences and Typology

### 10.1.1 Word Order Typology

- 언어마다 어순이 다름

![image](https://user-images.githubusercontent.com/80622859/204005853-514d9989-e843-4e2e-86a0-b99f6b2cd55f.png)

### 10.1.2 Lexical Divergences

- 번역은 문맥에 따라 적절한 단어가 달라질 수 있음
- 한 언어가 다른 언어보다 단어 선택에 더 많은 문법적 제약을 가할 수 있음
- Many-to-many mapping

### 10.1.3 Morphological Typology

- 단일 단어가 영어 전체 문장에 해당하는 매우 많은 형태소를 가질 수 있는 다합성 언어
- 형태소가 분할 가능할 정도로 상대적으로 깨끗한경계를 가진 융합 언어

### 10.1.4 Referential density

- 지시어 
- ex) 일본어와 중국어는 스페인어보다 훨씬 더 많이 생략하는 경향
- Cold language : 추론적인 작업을 더 많이 해야 하는 언어
- Hot language : 더 명확하고 듣는 사람이 더 쉽게 들을 수 있는 언어

## 10.2 The Encoder-Decoder Model

- 임의의 길이의 출력 시퀀스를 생성할 수 있는 모형
- 입력 시퀀스를 취하고 종종 context라고 불리는 맥락화된 표현을 만듦
- Context는 decode로 전달

![image](https://user-images.githubusercontent.com/80622859/204006577-913f49f1-70cf-459a-9e88-148a2998aa24.png)

## Encoder-Decoder with RNNs

![image](https://user-images.githubusercontent.com/80622859/204006825-b1afddc0-cd02-43ec-968f-b268720dfc6b.png)

- Encoder의 전체 목적은 입력의 상황에 맞는 표현을 생성
- 이러한 context는 enocoder의 마지막 은닉 상태에서 구현
- Decoder는 시퀀스 종료 마커가 생성될 때까지 한 번에 요소를 자동으로 생성함
- 각 은닉 상태는 이전 은닉 상태와 이전 상태에서 생성된 출력에 따라 조절

![image](https://user-images.githubusercontent.com/80622859/204007114-b3c7f5f6-7db3-492a-97de-a5c212b8a48a.png)

- 단점 : context가 시간이 지날수록 영향력이 감소함
- 위의 단점을 해결하기 위해서 각 단계에서 context vector c를 사용할 수 있게 함

![image](https://user-images.githubusercontent.com/80622859/204007195-c416633d-b4db-40aa-91e7-ee8e1d01848e.png)

- 마지막으로 softmax를 통해서 각 시간 단계에서 가장 가능성이 높은 출력을 계산

### 10.3.1 Training the Encoder-Decoder Model

- End-to-end
- 문장 세트와 그 번역으로 구성

![image](https://user-images.githubusercontent.com/80622859/204007408-409aa33c-da56-49af-aef8-0e58be3f6017.png)

- 교사 강제를 통해 학습

## 10.4 Attention

- Decoder가 마지막 은닉 상태뿐만 아니라 encoder의 숨겨진 모든 은닉 상태로부터 정보를 얻을 수 있도록 하는 방법
- 모든 encoder의 은닉 상태의 가중 합계를 취하여 단일 고정 길이 vector c를 만드는ㄱ ㅓㅅ
- Decoder가 현재 생성하고 있는 token과 관련된 source text의 특정 부분에 초점을 맞춤
- Attention은 static context vector를 decoder의 각 token에 대해 다른 encoder 은닉 상태에서 동적으로 파생된 vector 대체
- 이 context vector는 각 디코딩 단계 i와 함께 새로 생성되고 모든 인코더의 은닉 상태를 고려

![image](https://user-images.githubusercontent.com/80622859/204007872-e5c15bad-fedc-40db-b943-7422091eb8b4.png)

- $c_i$를 계산하는 첫 번째 단계는 각 encoder state에 얼마나 초점을 맞출 것인지, 각 encoder state가 $h^d_i$에서 얻은 decoder 상태와 얼마나 관련이 있는지 계산 
- Dot production attention score : 단순한 유사성을 계산
![image](https://user-images.githubusercontent.com/80622859/204008136-94cd2723-1254-4e85-88e8-626102f4736e.png)

![image](https://user-images.githubusercontent.com/80622859/204008156-6c509a07-bd14-4821-a29b-e9b938dfac9e.png)

- 마지막으로 $alpha$의 분포를 고려할 때, 모든 encoder hidden state에 대한 가중평균을 취함으로써 현재 decoder state에 대한 fixed length sequence vector를 계산

![image](https://user-images.githubusercontent.com/80622859/204008340-5fdd2a7a-4179-4938-9f74-bd3161a337d6.png)


- 동적 update

![image](https://user-images.githubusercontent.com/80622859/204008368-307b45e7-5263-4445-84b5-fa6f96797ff4.png)

10.5 Beam Search

- 평가값이 우수한 일정 개수의 확장 가능한 node만을 memory에 관리하면서 최상 우선 탐색을 적용하는 기법 
- Greedy decoding : 사후 판단으로 최고의 선택이 되었는지 여부에 관계없이 국소적으로 최적의 선택(각 단계에서 생성할 수 있는 가장 가능성이 높은 단일 토큰을 선택)
- 항상 최적은 아님
- 지금 decoder에게 잘 보이는 token이 나중에 잘못되었다는 것이 밝혀질 수 있음

- P(w_n|w_1, w_2, ..., w_n-1)을 모든 단어에 대해 구한 후 상위 K개의 단어만 남기고 나머지는 고려대상에서 제외
- 모든 경우에 대해 다 계산
- 최종적으로 K개의 sequence가 생성

## 10.6 Encoder-Decoder with Transformers

![image](https://user-images.githubusercontent.com/80622859/204009790-280edb0d-bda5-4e45-91c4-fc9a90415165.png)

![image](https://user-images.githubusercontent.com/80622859/204009838-792cb060-8f92-438f-ade1-b16243eeb565.png)

## 10.7 Some practical details on building MT systems

### 10.7.1 Tokenization

- 기계 번역은 일반적으로 고정된 어휘 사용
- BPE나 wordpiece algorithm 사용

## 10.8 MT Evaluation

1. 적절성: 번역문이 원문의 정확한 의미를 얼마나 잘 포착하는지. 때로는 충실함 또는 충실함이라고도 합니다.
2. 유창성: 번역이 대상 언어로 얼마나 유창한지(문법적이고, 명확하며, 읽기 쉽고, 자연스러운지).

### 10.8.1 Automatic Evaluation

#### Automatic Evaluation by Character Overlap: chrF

- Character F-score

![image](https://user-images.githubusercontent.com/80622859/204010275-302aaf2a-dfcb-46b9-945c-edeadeff2464.png)

- 일반적으로 재현율은 정밀도의 2배 비율로 가중함

![image](https://user-images.githubusercontent.com/80622859/204010353-aab8466d-c315-465e-b3c3-3eb2ed57db0a.png)

![image](https://user-images.githubusercontent.com/80622859/204010379-3878ea64-3079-4d5d-9430-4075d460d4c1.png)

- 공백 제거
- N-gram으로 쪼개서 계산

![image](https://user-images.githubusercontent.com/80622859/204010421-a72f3f79-4791-4046-bd0e-8db2275770ec.png)

- 전에는 단순 정밀도에 기반한 BLEU(BiLangual Evaluation Understudy) 사용
