{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/as9786/NLP/blob/main/Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_Ow4fTdwcTe"
      },
      "source": [
        "# Attention 신경망 구현 및 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bEGhVE2-aS-",
        "outputId": "69a8ff6b-0d45-48a2-f954-670de9bb7fc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.9/dist-packages (from konlpy) (4.9.2)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.9/dist-packages (from konlpy) (1.22.4)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 KB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from JPype1>=0.7.0->konlpy) (23.0)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iBst1OCSv-WH"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "from konlpy.tag import Okt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDDcOcLhwmdE"
      },
      "source": [
        "# 초매개변수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kGnJB0HiwlUB"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 200\n",
        "NUM_WORDS = 2000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YR0Dy7lwrAg"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Dp_wFCcCwqcK"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.emb = tf.keras.layers.Embedding(NUM_WORDS,64) \n",
        "    self.lstm = tf.keras.layers.LSTM(512,return_state=True,return_sequences=True)\n",
        "\n",
        "  def call(self,x,training=False,mask=None):\n",
        "    x = self.emb(x)\n",
        "    H,h,c = self.lstm(x)\n",
        "    return H,h,c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEGTPhmqxBFu"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "FuMmJiKLw7cX"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.emb = tf.keras.layers.Embedding(NUM_WORDS,64)\n",
        "    self.lstm = tf.keras.layers.LSTM(512,return_sequences=True,return_state=True)\n",
        "    self.att = tf.keras.layers.Attention() \n",
        "    self.dense = tf.keras.layers.Dense(NUM_WORDS,activation='softmax')\n",
        "\n",
        "  def call(self,inputs,training=False,mask=None): \n",
        "    x,s0,c0,H = inputs  # h와 c는 context\n",
        "    x = self.emb(x) \n",
        "    S, h, c = self.lstm(x,initial_state=[s0,c0])\n",
        "\n",
        "    s_ = tf.concat([s0[:,tf.newaxis,:],S[:,:-1,:]],axis=1)\n",
        "    A = self.att([s_,H])\n",
        "    y = tf.concat([S,A],axis=-1)\n",
        "    return self.dense(y),h,c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieucZwnYxdeG"
      },
      "source": [
        "# Seq2seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "4VZDPkbjxc3t"
      },
      "outputs": [],
      "source": [
        "class Seq2seq(tf.keras.Model):\n",
        "\n",
        "  def __init__(self,sos,eos):\n",
        "    super(Seq2seq,self).__init__()\n",
        "    self.enc = Encoder()\n",
        "    self.dec = Decoder()\n",
        "    self.sos = sos\n",
        "    self.eos = eos\n",
        "  \n",
        "  def call(self, inputs, training=False, mask=None): \n",
        "    if training is True:\n",
        "      x,y = inputs # 학습을 위해서는 정답 data도 같이 알아야 함\n",
        "      H,h,c = self.enc(x) # Hidden state, cell state\n",
        "      y, _, _ = self.dec((y,h,c,H))\n",
        "      return y #전체 문장장\n",
        "\n",
        "    else:\n",
        "      x = inputs # Test 시에는 정답이 있으면 안됨됨\n",
        "      H,h,c = self.enc(x)\n",
        "      y = tf.convert_to_tensor(self.sos) # 첫 번째 입력\n",
        "      y = tf.reshape(y,(1,1))\n",
        "\n",
        "      seq = tf.TensorArray(tf.int32,64)\n",
        "\n",
        "      for idx in tf.range(64):\n",
        "        y, h, c = self.dec([y,h,c,H])\n",
        "        y = tf.cast(tf.argmax(y,axis=-1),dtype=tf.int32)\n",
        "        y = tf.reshape(y,(1,1))\n",
        "        seq = seq.write(idx,y)\n",
        "\n",
        "        if y == self.eos:\n",
        "          break\n",
        "      return tf.reshape(seq.stack(),(1,64))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MB6N8M49xU8"
      },
      "source": [
        "# 학습, test loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "8JmvUwTNzdJP"
      },
      "outputs": [],
      "source": [
        "# Implement training loop\n",
        "@tf.function\n",
        "def train_step(model, inputs, labels, loss_object, optimizer, train_loss, train_accuracy):\n",
        "    output_labels = labels[:, 1:]\n",
        "    shifted_labels = labels[:, :-1]\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model([inputs, shifted_labels], training=True)\n",
        "        loss = loss_object(output_labels, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    train_loss(loss)\n",
        "    train_accuracy(output_labels, predictions)\n",
        "\n",
        "# Implement algorithm test\n",
        "@tf.function\n",
        "def test_step(model, inputs):\n",
        "    return model(inputs, training=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "StF-fbaI-YAo"
      },
      "outputs": [],
      "source": [
        "dataset_file = '/content/drive/MyDrive/chatbot_data.csv' # acquired from 'http://www.aihub.or.kr' and modified\n",
        "okt = Okt()\n",
        "\n",
        "with open(dataset_file, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "    seq = [' '.join(okt.morphs(line)) for line in lines]\n",
        "\n",
        "questions = seq[::2]\n",
        "answers = ['\\t ' + lines for lines in seq[1::2]]\n",
        "\n",
        "num_sample = len(questions)\n",
        "\n",
        "perm = list(range(num_sample))\n",
        "random.seed(0)\n",
        "random.shuffle(perm)\n",
        "\n",
        "train_q = list()\n",
        "train_a = list()\n",
        "test_q = list()\n",
        "test_a = list()\n",
        "\n",
        "for idx, qna in enumerate(zip(questions, answers)):\n",
        "    q, a = qna\n",
        "    if perm[idx] > num_sample//5:\n",
        "        train_q.append(q)\n",
        "        train_a.append(a)\n",
        "    else:\n",
        "        test_q.append(q)\n",
        "        test_a.append(a)\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=NUM_WORDS,\n",
        "                                                  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
        "\n",
        "tokenizer.fit_on_texts(train_q + train_a)\n",
        "\n",
        "train_q_seq = tokenizer.texts_to_sequences(train_q)\n",
        "train_a_seq = tokenizer.texts_to_sequences(train_a)\n",
        "\n",
        "test_q_seq = tokenizer.texts_to_sequences(test_q)\n",
        "test_a_seq = tokenizer.texts_to_sequences(test_a)\n",
        "\n",
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(train_q_seq,\n",
        "                                                        value=0,\n",
        "                                                        padding='pre',\n",
        "                                                        maxlen=64)\n",
        "y_train = tf.keras.preprocessing.sequence.pad_sequences(train_a_seq,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=65)\n",
        "\n",
        "\n",
        "x_test = tf.keras.preprocessing.sequence.pad_sequences(test_q_seq,\n",
        "                                                       value=0,\n",
        "                                                       padding='pre',\n",
        "                                                       maxlen=64)\n",
        "y_test = tf.keras.preprocessing.sequence.pad_sequences(test_a_seq,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=65)\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32).prefetch(1024)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(1).prefetch(1024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At7WDccqEPNP"
      },
      "source": [
        "# 학습 환경 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "HRb8hA9WEQQe"
      },
      "outputs": [],
      "source": [
        "# Create model\n",
        "model = Seq2seq(sos=tokenizer.word_index['\\t'],\n",
        "                eos=tokenizer.word_index['\\n'])\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Define performance metrics\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "h_sbh5H9EJJZ"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "49ZS6QWDFEbm"
      },
      "outputs": [],
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk3lz_IcFOAw"
      },
      "source": [
        "# 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HO2Vqj1rFNWS",
        "outputId": "7f6a4102-bd0c-4775-e4f0-70345d5f1ec3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.906686544418335, Accuracy: 83.16102600097656\n",
            "Epoch 2, Loss: 0.6184918880462646, Accuracy: 90.45661163330078\n",
            "Epoch 3, Loss: 0.5658604502677917, Accuracy: 91.19282531738281\n",
            "Epoch 4, Loss: 0.5496801137924194, Accuracy: 91.14974975585938\n",
            "Epoch 5, Loss: 0.5448196530342102, Accuracy: 91.14974975585938\n",
            "Epoch 6, Loss: 0.5266262292861938, Accuracy: 91.14974975585938\n",
            "Epoch 7, Loss: 0.5261322855949402, Accuracy: 91.21631622314453\n",
            "Epoch 8, Loss: 0.5129985809326172, Accuracy: 91.23590087890625\n",
            "Epoch 9, Loss: 0.4967699348926544, Accuracy: 91.39253997802734\n",
            "Epoch 10, Loss: 0.47819092869758606, Accuracy: 91.53743743896484\n",
            "Epoch 11, Loss: 0.4633048474788666, Accuracy: 91.81156158447266\n",
            "Epoch 12, Loss: 0.4567403197288513, Accuracy: 92.03868865966797\n",
            "Epoch 13, Loss: 0.4474528133869171, Accuracy: 92.285400390625\n",
            "Epoch 14, Loss: 0.4389043152332306, Accuracy: 92.35980224609375\n",
            "Epoch 15, Loss: 0.429273784160614, Accuracy: 92.50469970703125\n",
            "Epoch 16, Loss: 0.4283314347267151, Accuracy: 92.50861358642578\n",
            "Epoch 17, Loss: 0.41803306341171265, Accuracy: 92.57518768310547\n",
            "Epoch 18, Loss: 0.4141812324523926, Accuracy: 92.66525268554688\n",
            "Epoch 19, Loss: 0.405331015586853, Accuracy: 92.65742492675781\n",
            "Epoch 20, Loss: 0.40330949425697327, Accuracy: 92.75532531738281\n",
            "Epoch 21, Loss: 0.39244410395622253, Accuracy: 92.73966217041016\n",
            "Epoch 22, Loss: 0.3949943780899048, Accuracy: 92.85713958740234\n",
            "Epoch 23, Loss: 0.38472801446914673, Accuracy: 92.84539031982422\n",
            "Epoch 24, Loss: 0.3803410530090332, Accuracy: 92.9197998046875\n",
            "Epoch 25, Loss: 0.37199658155441284, Accuracy: 92.94721221923828\n",
            "Epoch 26, Loss: 0.376263290643692, Accuracy: 93.04903411865234\n",
            "Epoch 27, Loss: 0.3700251877307892, Accuracy: 93.04511260986328\n",
            "Epoch 28, Loss: 0.3623466491699219, Accuracy: 93.13909912109375\n",
            "Epoch 29, Loss: 0.36251381039619446, Accuracy: 93.15084838867188\n",
            "Epoch 30, Loss: 0.35908225178718567, Accuracy: 93.27616119384766\n",
            "Epoch 31, Loss: 0.35250383615493774, Accuracy: 93.24874877929688\n",
            "Epoch 32, Loss: 0.34997546672821045, Accuracy: 93.24874877929688\n",
            "Epoch 33, Loss: 0.3440212905406952, Accuracy: 93.3192367553711\n",
            "Epoch 34, Loss: 0.34182122349739075, Accuracy: 93.32315063476562\n",
            "Epoch 35, Loss: 0.3361011743545532, Accuracy: 93.40147399902344\n",
            "Epoch 36, Loss: 0.3340587913990021, Accuracy: 93.40930938720703\n",
            "Epoch 37, Loss: 0.32675614953041077, Accuracy: 93.44063568115234\n",
            "Epoch 38, Loss: 0.3209810256958008, Accuracy: 93.47979736328125\n",
            "Epoch 39, Loss: 0.31644853949546814, Accuracy: 93.58552551269531\n",
            "Epoch 40, Loss: 0.3125160336494446, Accuracy: 93.53462219238281\n",
            "Epoch 41, Loss: 0.3090755343437195, Accuracy: 93.58552551269531\n",
            "Epoch 42, Loss: 0.31105268001556396, Accuracy: 93.56594848632812\n",
            "Epoch 43, Loss: 0.3012121319770813, Accuracy: 93.73433685302734\n",
            "Epoch 44, Loss: 0.2984183132648468, Accuracy: 93.70692443847656\n",
            "Epoch 45, Loss: 0.2934664487838745, Accuracy: 93.7891616821289\n",
            "Epoch 46, Loss: 0.28681761026382446, Accuracy: 93.7891616821289\n",
            "Epoch 47, Loss: 0.28306612372398376, Accuracy: 93.8870620727539\n",
            "Epoch 48, Loss: 0.2804918587207794, Accuracy: 93.89097595214844\n",
            "Epoch 49, Loss: 0.27529364824295044, Accuracy: 93.98104858398438\n",
            "Epoch 50, Loss: 0.2731451690196991, Accuracy: 94.01237487792969\n",
            "Epoch 51, Loss: 0.26978904008865356, Accuracy: 94.03978729248047\n",
            "Epoch 52, Loss: 0.26331406831741333, Accuracy: 94.07502746582031\n",
            "Epoch 53, Loss: 0.2565479576587677, Accuracy: 94.22383880615234\n",
            "Epoch 54, Loss: 0.2522549033164978, Accuracy: 94.32957458496094\n",
            "Epoch 55, Loss: 0.24873322248458862, Accuracy: 94.49012756347656\n",
            "Epoch 56, Loss: 0.23977556824684143, Accuracy: 94.51362609863281\n",
            "Epoch 57, Loss: 0.23966342210769653, Accuracy: 94.70159912109375\n",
            "Epoch 58, Loss: 0.232821524143219, Accuracy: 94.69377136230469\n",
            "Epoch 59, Loss: 0.22547432780265808, Accuracy: 94.83474731445312\n",
            "Epoch 60, Loss: 0.22190724313259125, Accuracy: 94.99138641357422\n",
            "Epoch 61, Loss: 0.21979059278964996, Accuracy: 95.04621124267578\n",
            "Epoch 62, Loss: 0.21586206555366516, Accuracy: 95.18327331542969\n",
            "Epoch 63, Loss: 0.20894671976566315, Accuracy: 95.28508758544922\n",
            "Epoch 64, Loss: 0.20391729474067688, Accuracy: 95.42997741699219\n",
            "Epoch 65, Loss: 0.19748252630233765, Accuracy: 95.61795043945312\n",
            "Epoch 66, Loss: 0.19306468963623047, Accuracy: 95.79808807373047\n",
            "Epoch 67, Loss: 0.18799522519111633, Accuracy: 95.8607406616211\n",
            "Epoch 68, Loss: 0.18005402386188507, Accuracy: 96.09178924560547\n",
            "Epoch 69, Loss: 0.1739342361688614, Accuracy: 96.22493743896484\n",
            "Epoch 70, Loss: 0.169297456741333, Accuracy: 96.33067321777344\n",
            "Epoch 71, Loss: 0.16259919106960297, Accuracy: 96.44032287597656\n",
            "Epoch 72, Loss: 0.15961192548274994, Accuracy: 96.60479736328125\n",
            "Epoch 73, Loss: 0.15419985353946686, Accuracy: 96.69877624511719\n",
            "Epoch 74, Loss: 0.14749206602573395, Accuracy: 96.87891387939453\n",
            "Epoch 75, Loss: 0.14155761897563934, Accuracy: 96.97681427001953\n",
            "Epoch 76, Loss: 0.13525362312793732, Accuracy: 97.13737487792969\n",
            "Epoch 77, Loss: 0.12750835716724396, Accuracy: 97.32926177978516\n",
            "Epoch 78, Loss: 0.12291928380727768, Accuracy: 97.43498992919922\n",
            "Epoch 79, Loss: 0.11814169585704803, Accuracy: 97.5093994140625\n",
            "Epoch 80, Loss: 0.1151152178645134, Accuracy: 97.62687683105469\n",
            "Epoch 81, Loss: 0.1087866723537445, Accuracy: 97.77960968017578\n",
            "Epoch 82, Loss: 0.10407646745443344, Accuracy: 97.85792541503906\n",
            "Epoch 83, Loss: 0.10025486350059509, Accuracy: 97.98323822021484\n",
            "Epoch 84, Loss: 0.09661594033241272, Accuracy: 98.06156158447266\n",
            "Epoch 85, Loss: 0.09257519990205765, Accuracy: 98.05764770507812\n",
            "Epoch 86, Loss: 0.08959472179412842, Accuracy: 98.2338638305664\n",
            "Epoch 87, Loss: 0.08511681109666824, Accuracy: 98.32002258300781\n",
            "Epoch 88, Loss: 0.08049137890338898, Accuracy: 98.40225219726562\n",
            "Epoch 89, Loss: 0.07569468766450882, Accuracy: 98.45316314697266\n",
            "Epoch 90, Loss: 0.07297751307487488, Accuracy: 98.58631134033203\n",
            "Epoch 91, Loss: 0.06980179995298386, Accuracy: 98.61372375488281\n",
            "Epoch 92, Loss: 0.0677848756313324, Accuracy: 98.59806060791016\n",
            "Epoch 93, Loss: 0.06497984379529953, Accuracy: 98.76252746582031\n",
            "Epoch 94, Loss: 0.06307803839445114, Accuracy: 98.7899398803711\n",
            "Epoch 95, Loss: 0.05926089733839035, Accuracy: 98.80560302734375\n",
            "Epoch 96, Loss: 0.05688457936048508, Accuracy: 98.8682632446289\n",
            "Epoch 97, Loss: 0.05373232811689377, Accuracy: 98.96224975585938\n",
            "Epoch 98, Loss: 0.05022703856229782, Accuracy: 99.06015014648438\n",
            "Epoch 99, Loss: 0.04884463548660278, Accuracy: 99.08756256103516\n",
            "Epoch 100, Loss: 0.04592461884021759, Accuracy: 99.14630126953125\n",
            "Epoch 101, Loss: 0.04357045516371727, Accuracy: 99.22071075439453\n",
            "Epoch 102, Loss: 0.040711261332035065, Accuracy: 99.25203704833984\n",
            "Epoch 103, Loss: 0.03884277120232582, Accuracy: 99.30294799804688\n",
            "Epoch 104, Loss: 0.03741711750626564, Accuracy: 99.36168670654297\n",
            "Epoch 105, Loss: 0.035470541566610336, Accuracy: 99.38909912109375\n",
            "Epoch 106, Loss: 0.03448212891817093, Accuracy: 99.40084838867188\n",
            "Epoch 107, Loss: 0.032350704073905945, Accuracy: 99.42434692382812\n",
            "Epoch 108, Loss: 0.031756605952978134, Accuracy: 99.44783782958984\n",
            "Epoch 109, Loss: 0.029748855158686638, Accuracy: 99.4635009765625\n",
            "Epoch 110, Loss: 0.029106922447681427, Accuracy: 99.50658416748047\n",
            "Epoch 111, Loss: 0.029763562604784966, Accuracy: 99.51441192626953\n",
            "Epoch 112, Loss: 0.028703268617391586, Accuracy: 99.51832580566406\n",
            "Epoch 113, Loss: 0.026425758376717567, Accuracy: 99.5496597290039\n",
            "Epoch 114, Loss: 0.024773448705673218, Accuracy: 99.61231231689453\n",
            "Epoch 115, Loss: 0.023567108437418938, Accuracy: 99.6083984375\n",
            "Epoch 116, Loss: 0.022386711090803146, Accuracy: 99.62797546386719\n",
            "Epoch 117, Loss: 0.021440457552671432, Accuracy: 99.64363861083984\n",
            "Epoch 118, Loss: 0.02077878825366497, Accuracy: 99.62797546386719\n",
            "Epoch 119, Loss: 0.021732933819293976, Accuracy: 99.64363861083984\n",
            "Epoch 120, Loss: 0.020878124982118607, Accuracy: 99.67497253417969\n",
            "Epoch 121, Loss: 0.01978398486971855, Accuracy: 99.71412658691406\n",
            "Epoch 122, Loss: 0.018121570348739624, Accuracy: 99.71412658691406\n",
            "Epoch 123, Loss: 0.017069576308131218, Accuracy: 99.7454605102539\n",
            "Epoch 124, Loss: 0.016533134505152702, Accuracy: 99.76112365722656\n",
            "Epoch 125, Loss: 0.015716956928372383, Accuracy: 99.73371124267578\n",
            "Epoch 126, Loss: 0.014936638996005058, Accuracy: 99.77287292480469\n",
            "Epoch 127, Loss: 0.014406565576791763, Accuracy: 99.76112365722656\n",
            "Epoch 128, Loss: 0.013671042397618294, Accuracy: 99.78853607177734\n",
            "Epoch 129, Loss: 0.013296974822878838, Accuracy: 99.81202697753906\n",
            "Epoch 130, Loss: 0.013042843900620937, Accuracy: 99.83943939208984\n",
            "Epoch 131, Loss: 0.012372562661767006, Accuracy: 99.80419921875\n",
            "Epoch 132, Loss: 0.012071801349520683, Accuracy: 99.83552551269531\n",
            "Epoch 133, Loss: 0.011236114427447319, Accuracy: 99.83552551269531\n",
            "Epoch 134, Loss: 0.011047682724893093, Accuracy: 99.86685180664062\n",
            "Epoch 135, Loss: 0.010757053270936012, Accuracy: 99.82377624511719\n",
            "Epoch 136, Loss: 0.010576763190329075, Accuracy: 99.85902404785156\n",
            "Epoch 137, Loss: 0.010542640462517738, Accuracy: 99.82377624511719\n",
            "Epoch 138, Loss: 0.010187576524913311, Accuracy: 99.87860107421875\n",
            "Epoch 139, Loss: 0.009813861921429634, Accuracy: 99.85118865966797\n",
            "Epoch 140, Loss: 0.00987380649894476, Accuracy: 99.87077331542969\n",
            "Epoch 141, Loss: 0.010396713390946388, Accuracy: 99.83943939208984\n",
            "Epoch 142, Loss: 0.009511853568255901, Accuracy: 99.87468719482422\n",
            "Epoch 143, Loss: 0.009104026481509209, Accuracy: 99.88252258300781\n",
            "Epoch 144, Loss: 0.008552232757210732, Accuracy: 99.87468719482422\n",
            "Epoch 145, Loss: 0.009288907982409, Accuracy: 99.85118865966797\n",
            "Epoch 146, Loss: 0.008971288800239563, Accuracy: 99.8629379272461\n",
            "Epoch 147, Loss: 0.008175061084330082, Accuracy: 99.88252258300781\n",
            "Epoch 148, Loss: 0.007864005863666534, Accuracy: 99.88252258300781\n",
            "Epoch 149, Loss: 0.007567217573523521, Accuracy: 99.88252258300781\n",
            "Epoch 150, Loss: 0.0070840418338775635, Accuracy: 99.8942642211914\n",
            "Epoch 151, Loss: 0.006961729843169451, Accuracy: 99.902099609375\n",
            "Epoch 152, Loss: 0.006722797639667988, Accuracy: 99.8942642211914\n",
            "Epoch 153, Loss: 0.006481883116066456, Accuracy: 99.89035034179688\n",
            "Epoch 154, Loss: 0.00628422386944294, Accuracy: 99.9099349975586\n",
            "Epoch 155, Loss: 0.005979317240417004, Accuracy: 99.8942642211914\n",
            "Epoch 156, Loss: 0.00587833859026432, Accuracy: 99.88643646240234\n",
            "Epoch 157, Loss: 0.005879898089915514, Accuracy: 99.90601348876953\n",
            "Epoch 158, Loss: 0.005549599416553974, Accuracy: 99.9099349975586\n",
            "Epoch 159, Loss: 0.005413464270532131, Accuracy: 99.89818572998047\n",
            "Epoch 160, Loss: 0.005587885156273842, Accuracy: 99.902099609375\n",
            "Epoch 161, Loss: 0.005342699121683836, Accuracy: 99.89818572998047\n",
            "Epoch 162, Loss: 0.005026307888329029, Accuracy: 99.92559814453125\n",
            "Epoch 163, Loss: 0.005114604253321886, Accuracy: 99.9099349975586\n",
            "Epoch 164, Loss: 0.0049131340347230434, Accuracy: 99.90601348876953\n",
            "Epoch 165, Loss: 0.004841913003474474, Accuracy: 99.90601348876953\n",
            "Epoch 166, Loss: 0.004722034092992544, Accuracy: 99.91776275634766\n",
            "Epoch 167, Loss: 0.004727409686893225, Accuracy: 99.91384887695312\n",
            "Epoch 168, Loss: 0.0045047421008348465, Accuracy: 99.92167663574219\n",
            "Epoch 169, Loss: 0.0044749281369149685, Accuracy: 99.91384887695312\n",
            "Epoch 170, Loss: 0.005011968780308962, Accuracy: 99.8942642211914\n",
            "Epoch 171, Loss: 0.0053181941621005535, Accuracy: 99.902099609375\n",
            "Epoch 172, Loss: 0.005321227479726076, Accuracy: 99.91776275634766\n",
            "Epoch 173, Loss: 0.008434878662228584, Accuracy: 99.82377624511719\n",
            "Epoch 174, Loss: 0.019251037389039993, Accuracy: 99.62406158447266\n",
            "Epoch 175, Loss: 0.02376747876405716, Accuracy: 99.49091339111328\n",
            "Epoch 176, Loss: 0.019756197929382324, Accuracy: 99.6083984375\n",
            "Epoch 177, Loss: 0.013063570484519005, Accuracy: 99.81986236572266\n",
            "Epoch 178, Loss: 0.009850675240159035, Accuracy: 99.8629379272461\n",
            "Epoch 179, Loss: 0.008697601035237312, Accuracy: 99.85902404785156\n",
            "Epoch 180, Loss: 0.006719555240124464, Accuracy: 99.88252258300781\n",
            "Epoch 181, Loss: 0.005930720828473568, Accuracy: 99.9099349975586\n",
            "Epoch 182, Loss: 0.004961005877703428, Accuracy: 99.9099349975586\n",
            "Epoch 183, Loss: 0.004699578508734703, Accuracy: 99.92559814453125\n",
            "Epoch 184, Loss: 0.004221155308187008, Accuracy: 99.91776275634766\n",
            "Epoch 185, Loss: 0.004326395224779844, Accuracy: 99.89818572998047\n",
            "Epoch 186, Loss: 0.004239880479872227, Accuracy: 99.90601348876953\n",
            "Epoch 187, Loss: 0.004076286219060421, Accuracy: 99.92559814453125\n",
            "Epoch 188, Loss: 0.004004155285656452, Accuracy: 99.9099349975586\n",
            "Epoch 189, Loss: 0.003822452388703823, Accuracy: 99.91384887695312\n",
            "Epoch 190, Loss: 0.0038337945006787777, Accuracy: 99.92559814453125\n",
            "Epoch 191, Loss: 0.0036970083601772785, Accuracy: 99.91384887695312\n",
            "Epoch 192, Loss: 0.0037048272788524628, Accuracy: 99.91384887695312\n",
            "Epoch 193, Loss: 0.00356168276630342, Accuracy: 99.91384887695312\n",
            "Epoch 194, Loss: 0.0034870293457061052, Accuracy: 99.902099609375\n",
            "Epoch 195, Loss: 0.0033171814866364002, Accuracy: 99.92167663574219\n",
            "Epoch 196, Loss: 0.0034003895707428455, Accuracy: 99.91776275634766\n",
            "Epoch 197, Loss: 0.0032785185612738132, Accuracy: 99.91776275634766\n",
            "Epoch 198, Loss: 0.0031772679649293423, Accuracy: 99.92167663574219\n",
            "Epoch 199, Loss: 0.0032062234822660685, Accuracy: 99.9099349975586\n",
            "Epoch 200, Loss: 0.003318353556096554, Accuracy: 99.9099349975586\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    for seqs, labels in train_ds:\n",
        "        train_step(model, seqs, labels, loss_object, optimizer, train_loss, train_accuracy)\n",
        "\n",
        "    template = 'Epoch {}, Loss: {}, Accuracy: {}'\n",
        "    print(template.format(epoch + 1,\n",
        "                          train_loss.result(),\n",
        "                          train_accuracy.result() * 100))\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOswA3E3F95V",
        "outputId": "cd812dfa-c374-4bb7-80a6-4d7119d031fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_\n",
            "q:  ['여기 기프티콘 되죠 \\n']\n",
            "a:  ['\\t 네 현금영수증 해드릴까 요 \\n']\n",
            "p:  ['여기 카드 카드 역 면 사용 주시 입니다 \\n']\n",
            "_\n",
            "q:  ['네 에 테이크 아웃 도 가능한가요 \\n']\n",
            "a:  ['\\t 네 로 오시 면 테이크 아웃 잔 에 담아 드려요 \\n']\n",
            "p:  ['네 고객 님 시럽 는 아 주문 됩니다 \\n']\n",
            "_\n",
            "q:  ['아메리카노 톨 사이즈 로 주세요 \\n']\n",
            "a:  ['\\t 따뜻한 거 로 드릴 까요 \\n']\n",
            "p:  ['다른 건 필요 없으신 가요 \\n']\n",
            "_\n",
            "q:  ['진동 을 따로 주시나요 \\n']\n",
            "a:  ['\\t 주 번호 로 드리겠습니다 \\n']\n",
            "p:  ['스콘 은 데워 드릴 까요 \\n']\n",
            "_\n",
            "q:  ['자리 있나요 \\n']\n",
            "a:  ['\\t 네 있습니다 \\n']\n",
            "p:  ['아니요 치즈케이크 는 지금 없어요 \\n']\n",
            "_\n",
            "q:  ['그럼 루이보스 밀크 티 하나 \\n']\n",
            "a:  ['\\t 네 알겠습니다 \\n']\n",
            "p:  ['네 4500원 입니다 \\n']\n",
            "_\n",
            "q:  ['다음 에 무료 로 하고 엔 도장 찍어주세요 \\n']\n",
            "a:  ['\\t 네 \\n']\n",
            "p:  ['네 쿠폰 만 주시 면 적립 드리겠습니다 \\n']\n",
            "_\n",
            "q:  ['아메리카노 한 잔 에 얼마 죠 \\n']\n",
            "a:  ['\\t 입니다 \\n']\n",
            "p:  ['4000원 입니다 \\n']\n",
            "_\n",
            "q:  ['얼마나 \\n']\n",
            "a:  ['\\t 바로 만들어 드릴게요 \\n']\n",
            "p:  ['10분 정도 걸려요 \\n']\n",
            "_\n",
            "q:  ['카푸치노 는 로 주시 고 아메리카노 는 로 \\n']\n",
            "a:  ['\\t 네 더 없으세요 \\n']\n",
            "p:  ['아이스 아메리카노 한잔 주문 도 와 드리겠습니다 \\n']\n",
            "_\n",
            "q:  ['아메리카노 는 어떤 종류 가 있나요 \\n']\n",
            "a:  ['\\t 디카 페인 과 기본 아메리카노 2 종류 있습니다 \\n']\n",
            "p:  ['네 고객 님 티 종류 다 가능해요 \\n']\n",
            "_\n",
            "q:  ['카카오 페이 로 결제 가능한가요 \\n']\n",
            "a:  ['\\t 네 가능합니다 \\n']\n",
            "p:  ['네 있습니다 \\n']\n",
            "_\n",
            "q:  ['오늘 의 커피 는 커피 로 하나요 맛 이 \\n']\n",
            "a:  ['\\t 아 네 오늘 은 과테말라 커피 입니다 \\n']\n",
            "p:  ['네 과테말라 와 케냐 를 섞은 마이크로 블렌드 를 추천 하겠습니다 \\n']\n",
            "_\n",
            "q:  ['머핀 은 뭐 가 제일 \\n']\n",
            "a:  ['\\t 블루베리 머핀 이 잘 나갑니다 \\n']\n",
            "p:  ['치즈 은 시럽 가 잘 제일']\n",
            "_\n",
            "q:  ['현금 영수증 해주세요 \\n']\n",
            "a:  ['\\t 네 번호 찍어주세요 \\n']\n",
            "p:  ['네 번호 찍어주세요 \\n']\n",
            "_\n",
            "q:  ['둘 다 톨 사이즈 로 주세요 \\n']\n",
            "a:  ['\\t 여기 서 드시고 요 \\n']\n",
            "p:  ['드시고 가시나요 \\n']\n",
            "_\n",
            "q:  ['아이스 아메리카노 한 잔 가능한가요 \\n']\n",
            "a:  ['\\t 네 가능합니다 \\n']\n",
            "p:  ['네 가능합니다만 말차 에서는 는 품절 되었습니다 \\n']\n",
            "_\n",
            "q:  ['아이스 아메리카노 에 샷 이 몇 개 \\n']\n",
            "a:  ['\\t 아이스 아메리카노 에 샷 은 개 \\n']\n",
            "p:  ['네 4000원 입니다 \\n']\n",
            "_\n",
            "q:  ['카페라테 한 잔 주세요 \\n']\n",
            "a:  ['\\t 카페라테 따뜻한 걸 로 드릴 까요 \\n']\n",
            "p:  ['네 카페라떼 컵 사이즈 는 뭘 로 드릴 까요 \\n']\n",
            "_\n",
            "q:  ['아니요 \\n']\n",
            "a:  ['\\t 네 더 필요하신 건 없으신 가요 \\n']\n",
            "p:  ['쓴맛 은 많이 없고 산미 가 있고 다른 원두 들 보다 과일 향 이 나죠 \\n']\n",
            "_\n",
            "q:  ['네 찍어주세요 \\n']\n",
            "a:  ['\\t 네 주문 딸기 스무디 와 쿠키 드릴게요 \\n']\n",
            "p:  ['네 더 필요하신 건 없으세요 \\n']\n",
            "_\n",
            "q:  ['시즌 메뉴 오늘 도 가능한가요 \\n']\n",
            "a:  ['\\t 네 시즌 메뉴 가능합니다 \\n']\n",
            "p:  ['아뇨 현재 법적 으로 금지 하고 있어요 \\n']\n",
            "_\n",
            "q:  ['시즌 메뉴 와 함께 되어 있는 세트 메뉴 가 있나요 \\n']\n",
            "a:  ['\\t 네 치즈 케이크 와 시즌 메뉴 두 잔 으로 세트 메뉴 있습니다 \\n']\n",
            "p:  ['네 자리 있습니다']\n",
            "_\n",
            "q:  ['라테 에 우유 두 도 변경 가능한가요 \\n']\n",
            "a:  ['\\t 네 라테 에 두유 로 변경 가능합니다 \\n']\n",
            "p:  ['아뇨 매장 에서는 머그컵 만 사용 가능합니다 \\n']\n",
            "_\n",
            "q:  ['네 먹고 갈 거 예요 \\n']\n",
            "a:  ['\\t 카드 여기 주세요 \\n']\n",
            "p:  ['네 그럼 바로 거 유리잔 \\n']\n",
            "_\n",
            "q:  ['카페인 이 음료 있나요 \\n']\n",
            "a:  ['\\t 티 음료 와 스무디 에는 카페인 이 않습니다 \\n']\n",
            "p:  ['네 자리 있습니다']\n",
            "_\n",
            "q:  ['딸기스무디 랑 키위 스무디 는 생 과일 인가요 \\n']\n",
            "a:  ['\\t 딸기 는 키위 는 생 과일 을 사용 하고 있습니다 \\n']\n",
            "p:  ['요새 는 케이크 가 아이스 카페라테 의 아이스 걸 로 드릴 까요 \\n']\n",
            "_\n",
            "q:  ['그럼 딸기 스무디 하나 주세요 \\n']\n",
            "a:  ['\\t 드시고 가시나요 \\n']\n",
            "p:  ['네 더 필요하신 건 없으세요 \\n']\n",
            "_\n",
            "q:  ['아메리카노 한 잔이요 \\n']\n",
            "a:  ['\\t 아이스 아메리카노 로 드릴 까요 \\n']\n",
            "p:  ['드시고 가시나요 \\n']\n",
            "_\n",
            "q:  ['네 도 같이 \\n']\n",
            "a:  ['\\t 네 아메리카노 4000원 입니다 \\n']\n",
            "p:  ['오늘 은 다 팔렸어요 \\n']\n",
            "_\n",
            "q:  ['디카 페인 아이스 아메리카노 한 잔 주세요 \\n']\n",
            "a:  ['\\t 디카 페인 아이스 아메리카노 는 기존 금액 에 300원 추가 되는데 괜찮으신 가요 \\n']\n",
            "p:  ['네 카페라떼 컵 하실 것 은 없으신 가요 \\n']\n",
            "_\n",
            "q:  ['커피 음료 것 뭐 가 있나요 \\n']\n",
            "a:  ['\\t 스무디 와 주스 있습니다 \\n']\n",
            "p:  ['네 자리 있습니다']\n",
            "_\n",
            "q:  ['주스 어떤 종류 있나요 \\n']\n",
            "a:  ['\\t 딸기 주스 주스 주스 가 있습니다 \\n']\n",
            "p:  ['네 그럼 비 쿠폰 있으세요 \\n']\n",
            "_\n",
            "q:  ['플랫 화이트 라지 로 주세요 \\n']\n",
            "a:  ['\\t 네 \\n']\n",
            "p:  ['커피 는 시럽 은 넣어 드릴 까요 \\n']\n",
            "_\n",
            "q:  ['네 레드 벨벳 케이크 주세요 \\n']\n",
            "a:  ['\\t 음료 는 뭘 로 드릴 까요 \\n']\n",
            "p:  ['네 총 8000원 입니다 \\n']\n",
            "_\n",
            "q:  ['네 먹고 갈 거 예요 \\n']\n",
            "a:  ['\\t 유리잔 괜찮으세요 \\n']\n",
            "p:  ['네 그럼 바로 거 유리잔 \\n']\n",
            "_\n",
            "q:  ['따뜻한 밀크 티 주세요 \\n']\n",
            "a:  ['\\t 네 \\n']\n",
            "p:  ['네 단체 주문 은 재료 추가 불가능합니다 \\n']\n",
            "_\n",
            "q:  ['음료 얼마나 하나요 \\n']\n",
            "a:  ['\\t 10분 정도 주시 면 됩니다 \\n']\n",
            "p:  ['초코 때 말씀 해주시면 테이크 아웃 잔 은 드릴 테 니 우선 머그잔 에 드리겠습니다 \\n']\n",
            "_\n",
            "q:  ['아이스 아메리카노 한잔 얼마 인가요 \\n']\n",
            "a:  ['\\t 4500원 입니다 \\n']\n",
            "p:  ['아이스 아메리카노 1 잔 은 4000원 아이스 카페라테 1 잔 은 5000원 입니다 \\n']\n",
            "_\n",
            "q:  ['현금영수증 번호 \\n']\n",
            "a:  ['\\t 네 \\n']\n",
            "p:  ['네 번호 입력 해주세요 \\n']\n",
            "_\n",
            "q:  ['이 카드 로 결제 해주세요 \\n']\n",
            "a:  ['\\t 네 결제 도 와 드릴게요 \\n']\n",
            "p:  ['네 알겠습니다 \\n']\n",
            "_\n",
            "q:  ['주문 한 게 다 안 \\n']\n",
            "a:  ['\\t 주 번호 가 몇 이 죠 \\n']\n",
            "p:  ['네 감사합니다 \\n']\n",
            "_\n",
            "q:  ['을 \\n']\n",
            "a:  ['\\t \\n']\n",
            "p:  ['쓴맛 은 많이 없고 산미 가 다른 다른 원두 가 제일 나죠 \\n']\n",
            "_\n",
            "q:  ['베이글 은 얼마 인가요 \\n']\n",
            "a:  ['\\t 베이글 은 2000원 입니다 \\n']\n",
            "p:  ['플랫 화이트 4500원 입니다 \\n']\n",
            "_\n",
            "q:  ['지금 되나요 \\n']\n",
            "a:  ['\\t 는 계절 메뉴 라 지금 은 판매 하지 않습니다 \\n']\n",
            "p:  ['와이파이 암호 는 가나다라 입니다 \\n']\n",
            "_\n",
            "q:  ['바닐라 라테 는 따뜻하게 주세요 \\n']\n",
            "a:  ['\\t 네 적립 이나 할인 카드 있으세요 \\n']\n",
            "p:  ['네 알겠습니다 \\n']\n",
            "_\n",
            "q:  ['테이크 아웃 으로 부탁드립니다 \\n']\n",
            "a:  ['\\t 결제 는 이 쪽 에서 도 와 드릴게요 \\n']\n",
            "p:  ['네 진동 벨 울리면 픽업 테이블 로 와주세요 \\n']\n",
            "_\n",
            "q:  ['혹시 테이크 아웃 잔 에 수 있나요 \\n']\n",
            "a:  ['\\t 테이크 아웃 하시는 건가 요 \\n']\n",
            "p:  ['네 저기 서 직접 가져오시면 됩니다 \\n']\n",
            "_\n",
            "q:  ['아메리카노 하나 는 샷 추가 해주세요 \\n']\n",
            "a:  ['\\t 아메리카노 는 둘 다 따뜻한 걸 로 드릴 까요 \\n']\n",
            "p:  ['네 저희 님 준비 해드리겠습니다 \\n']\n",
            "_\n",
            "q:  ['쿠폰 찍어주세요 \\n']\n",
            "a:  ['\\t 네 찍어 드릴게요 \\n']\n",
            "p:  ['10 개 다모아 오시 면 커피한잔 드려요 \\n']\n",
            "_\n",
            "q:  ['주문 할게요 \\n']\n",
            "a:  ['\\t 어떤 거 드릴 까요 \\n']\n",
            "p:  ['네 어떤 걸 로 하시겠습니까 \\n']\n",
            "_\n",
            "q:  ['파나요 \\n']\n",
            "a:  ['\\t 는 계절 지금 은 \\n']\n",
            "p:  ['오늘 은 다 팔렸어요 \\n']\n",
            "_\n",
            "q:  ['그럼 겨울 메뉴 뭐 가 \\n']\n",
            "a:  ['\\t 겨울 엔 감귤 라테 가 제일 많이 나가요 \\n']\n",
            "p:  ['치즈 은 시럽 가 잘 제일']\n",
            "_\n",
            "q:  ['네 결제 는 카드 로 할게요 \\n']\n",
            "a:  ['\\t 네 결제 완료 되었습니다 \\n']\n",
            "p:  ['네 포인트 적립 결제 있으세요 \\n']\n",
            "_\n",
            "q:  ['둘 다 사이즈 로 할게요 \\n']\n",
            "a:  ['\\t 네 결제 는 어떤 것 으로 도 와 드릴 까요 \\n']\n",
            "p:  ['드시고 가세 요 \\n']\n",
            "_\n",
            "q:  ['기프티콘 으로 결제 할게요 \\n']\n",
            "a:  ['\\t 네 그럼 쿠폰 저 \\n']\n",
            "p:  ['저 한테 보여주시고 제 가 확인 버튼 누르면 돼요 \\n']\n",
            "_\n",
            "q:  ['녹차 라테 1 잔 주세요 \\n']\n",
            "a:  ['\\t 따뜻한 걸 로 드릴 까요 \\n']\n",
            "p:  ['드시고 가시나요 \\n']\n",
            "_\n",
            "q:  ['네 그럼 휘핑크림 추가 해서 주세요 \\n']\n",
            "a:  ['\\t 네 녹차 라테 에 휘핑크림 추가 해서 4500원 입니다 \\n']\n",
            "p:  ['드시고 가시나요 \\n']\n",
            "_\n",
            "q:  ['브레드 종류 는 뭐 가 있나요 \\n']\n",
            "a:  ['\\t 허니 브레드 와 갈릭 치즈 브레드 가 있습니다 \\n']\n",
            "p:  ['네 500원 있습니다 \\n']\n",
            "_\n",
            "q:  ['생크림 이 건 어떤 건가 요 \\n']\n",
            "a:  ['\\t 허니 브레드 입니다 \\n']\n",
            "p:  ['네 고객 님 결제 완료 되었습니다 \\n']\n",
            "_\n",
            "q:  ['네 그렇게 만들어 주세요 \\n']\n",
            "a:  ['\\t 더 필요한 건 없으세요 \\n']\n",
            "p:  ['드시고 가실 건가 요 \\n']\n",
            "_\n",
            "q:  ['여기 있습니다 \\n']\n",
            "a:  ['\\t 네 확인 되셨고 되면 진동 벨 거 예요 \\n']\n",
            "p:  ['손님 지금 개 포인트 있으신 데 사용 해 드릴 까요 \\n']\n",
            "_\n",
            "q:  ['핫초코 한 잔 아메리카노 사이 즈 업 한 잔 하면 얼마 인가요 \\n']\n",
            "a:  ['\\t 입니다 \\n']\n",
            "p:  ['5000원 입니다 \\n']\n",
            "_\n",
            "q:  ['주스 는 다른 건 없나요 \\n']\n",
            "a:  ['\\t 그럼 에 라테 추천 \\n']\n",
            "p:  ['네 고객 님 티 종류 다 아이스 가능합니다 \\n']\n",
            "_\n",
            "q:  ['그건 \\n']\n",
            "a:  ['\\t 네 만 따듯 해 요 \\n']\n",
            "p:  ['10분 정도 걸려요 \\n']\n",
            "_\n",
            "q:  ['통신사 할인 되죠 \\n']\n",
            "a:  ['\\t 네 300원 할인 됩니다 \\n']\n",
            "p:  ['네 더 필요하신 건 없으세요 \\n']\n",
            "_\n",
            "q:  ['매장 에서 언제 까지 영업 하시나요 \\n']\n",
            "a:  ['\\t 오후 10시 까지 영업 입니다 \\n']\n",
            "p:  ['네 자몽 차는 있습니다 \\n']\n",
            "_\n",
            "q:  ['아니요 그냥 주세요 \\n']\n",
            "a:  ['\\t 결제 해드릴게요 \\n']\n",
            "p:  ['카페모카 5천 원 입니다 \\n']\n",
            "_\n",
            "q:  ['가격 안 되나요 \\n']\n",
            "a:  ['\\t 한 해드릴게요 \\n']\n",
            "p:  ['네 \\n']\n",
            "_\n",
            "q:  ['카페라테 한잔 주세요 \\n']\n",
            "a:  ['\\t 따뜻한 걸 로 드릴 까요 \\n']\n",
            "p:  ['네 드시고 가시나요 \\n']\n",
            "_\n",
            "q:  ['네 차가운 걸 로 주세요 \\n']\n",
            "a:  ['\\t 4500원 입니다 \\n']\n",
            "p:  ['드시고 가시나요 \\n']\n",
            "_\n",
            "q:  ['어떤 게 괜찮아요 \\n']\n",
            "a:  ['\\t 이나 원두 를 하시는 게 아니면 예 가체 많이 추천 \\n']\n",
            "p:  ['네 고객 님 티 종류 다 아이스 가능합니다 \\n']\n",
            "_\n",
            "q:  ['그럼 추천 치즈 케이크 도 같이 주세요 \\n']\n",
            "a:  ['\\t 네 매장 에서 드시고 가시나요 \\n']\n",
            "p:  ['초코 머핀 데워 드릴 까요 \\n']\n",
            "_\n",
            "q:  ['그리고 휘핑크림 은 에스프레소 크림 으로 \\n']\n",
            "a:  ['\\t 결제 는 어떻게 해드릴까 요 \\n']\n",
            "p:  ['네 그럼 시럽 많이 넣어 드릴게요 \\n']\n",
            "_\n",
            "q:  ['지금 도 할인 하나요 \\n']\n",
            "a:  ['\\t 네 10시 까지 하고 있습니다 \\n']\n",
            "p:  ['네 쿠폰 찍어 도 와 드리겠습니다 \\n']\n",
            "_\n",
            "q:  ['그럼 와 아이스 아메리카노 로 할게요 \\n']\n",
            "a:  ['\\t 더 필요하신 건 없나요 \\n']\n",
            "p:  ['네 아이스 컵 는 뭘 로 하시겠어요 \\n']\n",
            "_\n",
            "q:  ['네 할인 적립 은 \\n']\n",
            "a:  ['\\t 네 바코드 \\n']\n",
            "p:  ['네 사이즈 는 어떤 걸 로 주문 넣어 드릴 까요 \\n']\n",
            "_\n",
            "q:  ['초코 프라푸치노 주세요 \\n']\n",
            "a:  ['\\t 휘핑 올려 드릴 까요 \\n']\n",
            "p:  ['네 더 컵 사이즈 는 뭘 로 드릴 까요 \\n']\n",
            "_\n",
            "q:  ['시럽 도 가 \\n']\n",
            "a:  ['\\t 드시고 가시나요 \\n']\n",
            "p:  ['네 진동 벨 로 픽업 테이블 로 와주세요 \\n']\n",
            "_\n",
            "q:  ['둘 다 사이즈 로 주세요 \\n']\n",
            "a:  ['\\t 드시고 가시나요 \\n']\n",
            "p:  ['드시고 가시나요 \\n']\n",
            "_\n",
            "q:  ['마시다가 갈 건데 테이크아웃 으로 주세요 \\n']\n",
            "a:  ['\\t 상 매장 에서는 머그컵 으로 드리고 있어요 \\n']\n",
            "p:  ['네 알겠습니다 \\n']\n",
            "_\n",
            "q:  ['나갈 때 테이크아웃 컵 으로 수 있나요 \\n']\n",
            "a:  ['\\t 네 그렇게 해드릴게요 \\n']\n",
            "p:  ['네 캐리어 에 담아 드릴게요 \\n']\n",
            "_\n",
            "q:  ['아메리카노 두 잔 한잔 주세요 \\n']\n",
            "a:  ['\\t 드시고 가실 건가 요 \\n']\n",
            "p:  ['드시고 가시나요 \\n']\n",
            "_\n",
            "q:  ['얼마나 하나요 \\n']\n",
            "a:  ['\\t 5분 정도 \\n']\n",
            "p:  ['10분 정도 걸려요 \\n']\n",
            "_\n",
            "q:  ['포인트 적립 해주세요 \\n']\n",
            "a:  ['\\t 네 번호 입력 부탁드립니다 \\n']\n",
            "p:  ['네 기다리시면 안내 해드릴게요 \\n']\n",
            "_\n",
            "q:  ['네 번호 로 할게요 \\n']\n",
            "a:  ['\\t 네 에 번호 \\n']\n",
            "p:  ['현금영수증 해드릴까 요 \\n']\n",
            "_\n",
            "q:  ['아 포인트 포인트 사용 해주세요 \\n']\n",
            "a:  ['\\t 네 고객 님 포인트 총 있으신 데 사용 도 와 드리겠습니다 \\n']\n",
            "p:  ['네 알겠습니다 \\n']\n",
            "_\n",
            "q:  ['톨 사이즈 로 주문 할게요 \\n']\n",
            "a:  ['\\t 네 계산 도 와 드리겠습니다 \\n']\n",
            "p:  ['네 어떤 걸 로 드릴 까요 \\n']\n",
            "_\n",
            "q:  ['아메리카노 사이즈 가능한가요 \\n']\n",
            "a:  ['\\t 네 500원 만 추가 하시면 가능하십니다 \\n']\n",
            "p:  ['네 가능합니다 \\n']\n",
            "_\n",
            "q:  ['사이 즈 업 해서 주세요 \\n']\n",
            "a:  ['\\t 네 결제 는 어떻게 도 와 드릴 까요 \\n']\n",
            "p:  ['네 사이즈 는 어떻게 드릴 까요 \\n']\n",
            "_\n",
            "q:  ['커피 는 텀블러 에 담아주세요 \\n']\n",
            "a:  ['\\t 네 텀블러 할인 4000원 결제 도 와 드리겠습니다 \\n']\n",
            "p:  ['고객 님 텀블러 이용 하시면 500원 할인 가능합니다 \\n']\n",
            "_\n",
            "q:  ['아니요 아이스 로 주세요 \\n']\n",
            "a:  ['\\t 드시고 가실 건가 요 \\n']\n",
            "p:  ['레귤러 사이즈 로 괜찮으세요 \\n']\n",
            "_\n",
            "q:  ['테이크아웃 할게요 \\n']\n",
            "a:  ['\\t 지금 중 인데 케이크 주문 하시면 아메리카노 한잔 로 드려요 \\n']\n",
            "p:  ['네 드릴게요 잠시 만 요 \\n']\n",
            "_\n",
            "q:  ['현금 결제 가 안 \\n']\n",
            "a:  ['\\t 현금 은 에서 주문 도 와 드리겠습니다 \\n']\n",
            "p:  ['네 4500원 입니다 \\n']\n",
            "_\n",
            "q:  ['포인트 적립 되나요 \\n']\n",
            "a:  ['\\t 번호 포인트 적립 도 와 드리고 있어요 \\n']\n",
            "p:  ['네 \\n']\n",
            "_\n",
            "q:  ['포인트 적립 할게요 \\n']\n",
            "a:  ['\\t 네 결제 되셨습니다 \\n']\n",
            "p:  ['네 잠시 만 기다려주세요 \\n']\n",
            "_\n",
            "q:  ['티라미수 는 있나요 \\n']\n",
            "a:  ['\\t 네 티라미수 는 있습니다 \\n']\n",
            "p:  ['네 있습니다 \\n']\n",
            "_\n",
            "q:  ['네 현금영수증 해주세요 \\n']\n",
            "a:  ['\\t 네 드시고 가시나요 \\n']\n",
            "p:  ['네 번호 입력 해주세요 \\n']\n",
            "_\n",
            "q:  ['샷 추가 해주세요 \\n']\n",
            "a:  ['\\t 네 알겠습니다 \\n']\n",
            "p:  ['네 결제 도 와 드리겠습니다 \\n']\n",
            "_\n",
            "q:  ['얼마 에요 \\n']\n",
            "a:  ['\\t 만 원 입니다 \\n']\n",
            "p:  ['총 7000원 입니다 \\n']\n",
            "_\n",
            "q:  ['아이스 아메리카노 랑 샌드위치 주세요 \\n']\n",
            "a:  ['\\t 10시 에 세트 할인 가능하세요 \\n']\n",
            "p:  ['아메리카노 아메리카노 아이스 와 원두 테이크아웃 원두 테이크아웃 드릴 까요 \\n']\n"
          ]
        }
      ],
      "source": [
        "for test_seq, test_labels in test_ds:\n",
        "    prediction = test_step(model, test_seq)\n",
        "    test_text = tokenizer.sequences_to_texts(test_seq.numpy())\n",
        "    gt_text = tokenizer.sequences_to_texts(test_labels.numpy())\n",
        "    texts = tokenizer.sequences_to_texts(prediction.numpy())\n",
        "    print('_')\n",
        "    print('q: ', test_text)\n",
        "    print('a: ', gt_text)\n",
        "    print('p: ', texts)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gQnJORD-YLr2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1iinGtFEuRQ5wqJ1cEOd5GOcx6kg9Mah-",
      "authorship_tag": "ABX9TyPKLSlKNXWSWPsOdDFEZqzI",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}