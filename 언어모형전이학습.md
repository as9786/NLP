# Transfer Learning with Pretrained Language Models and Contextual Embeddings

## 1.Bidirectional Transformer Encoders

- Bidirectional encoder는 self-attention이 전체 입력에 걸쳐 범위를 갖도록 함

![image](https://user-images.githubusercontent.com/80622859/204095708-946f53d1-35e7-4db3-b495-2a29e96d5bb8.png)

![image](https://user-images.githubusercontent.com/80622859/204095759-39c23850-4627-428c-bae7-3bed7a8a8c73.png)

### BERT

- 768 크기의 은닉층
- 12개의 multi-head-attention
- 100M 이상의 매개 변수를 가진 모형
- WordPiece 기반 : 하위 단어 token에 기반을 두고 있음
- 모든 입력 문장은 먼저 토큰화되어야 하고, 그 다음 모든 추가 처리는 단어가 아닌 하위 단어 token에서 이루어짐
- 512개의 하위 단어 토큰의 고정 입력 크기가 사용


## 2. Training Bidirectional Encoders
- 빈칸 채우기 작업을 수행하는 방법을 학습
- 하나 이상의 요소가 누락된 입력 시퀀스가 주어지면 학습 과제는 누락된 요소를 예측하는 것
- 훈련 중에 모형은 입력 시퀀스의 하나 이상의 요소를 지우고 지워진 각 항목에 대한 어휘 확률 분포를 생성
- 각 모형의 예측에서 cross entropy loss function을 사용하여 학습
- 훈련 data를 일부로 손상시킨 다음 모형에게 원래 입력을 복구하도록 요청하는 학습 방법
- Mask, 대체, 순서 변경, 삭제 등

### 1. Masking Words
- Bidirectional encoder를 훈련하는 원래의 접근 방식은 masking language modeling(MLM)이라고 함
- 큰 말뭉치의 주석이 없는 text 사용
- 각 훈련 시퀀스의 token sample을 무작위로 선택
- 고유한 어휘 token은 [MASK]로 대체
- 다른 token으로 대체
- 변경 X
- Input token 중 15%는 학습을 위해 sampling
- 이 중에서 80%sms [MASK]로 대체되고, 10%는 무작위로 선택된 token으로 대체되며, 나머지 10% 변경되지 않은 상태
- Masking된 token에 대한 원래 입력을 예측하는 것

![image](https://user-images.githubusercontent.com/80622859/204096144-8a6b0f55-2c1d-48e6-aeeb-f480c35dcae3.png)

### 2. Masking Spans
- Span : 단어 토큰화 이전에 train text에서 선택된 하나 이상의 단어로 구성된 연속된 sequence
- SpanBERT
- BERT와 마찬가지로 MLM 학습
- Span의 어떤 단어가 예측되고 있는지를 알려 주는 positional embedding과 함께 masking span의 바로 앞과 뒤에 있는 단어와 연관된 output vector를 사용하여 이루어짐 
- 최종 손실은 BERT MLM 손실과 SBO(Span Boundary Objective) 손실의 합
- SBO : Masking span 내의 단어를 바로 앞과 뒤의 단어로부터 예측하는 모형

![image](https://user-images.githubusercontent.com/80622859/204096380-54703ce4-4ccf-4c57-a7c6-3069fa497d91.png)

### 3. Next Sentence Prediction

- Masking based training : 효과적인 단어 수준 표현을 생성하는 것이 목적
- 문장 쌍 간의 관계를 결정하는 것도 학습해야 함
- Pair phrase extraction(두 문장이 유사한 의미를 갖는지), entailment(두 문장의 의미가 서로 모순되는지), discource coherence(두 이웃 문장이 일관된 담론을 형성하는지)
- Next Sentence Prediction(NSP)
- 실제 인접 문장 쌍으로 구성되는지 아니면 관련이 없는 문장 쌍으로 구성되는 예측
- 반은 제대로 반은 무작위로 배치
- [CLS], [SEP] tokens
- [CLS] : 입력 문장 쌍 앞에 추가
- [SEP] : 문장 사이와 두 번째 문장의 최종 token 뒤에 배치
- 위의 tokens도 embedding

![image](https://user-images.githubusercontent.com/80622859/204096608-f4d0bb36-d110-4a8f-9ab0-9a8a3b17daf3.png)

### 4. Training Regimes
- 8억 단어 말뭉치와 25억 단어 말뭉치로 구성

### 5. Contextual Embedding
- 모형의 출력이 입력의 각 token에 대한 상황별 embedding을 구성한다고 생각할 수 있음
- 마지막 4개 층 각각에서의 출력값 => 하나의 단어 vector

## 3. Transfer Learning through Fine-Tuning
