# Exploring the Limits of Transfer Learning with a Unifed Text-to-Text Transformer

## 목적
- 최근 NLP 연구들은 대량의 unsupervised dataset에 대해 사전훈련된 모형을 목표로 하는 downstream task에 대해 지도 학습을 통한 미세 조정을 하는 전이 학습 방식이 보편화
- 위와 같은 방식이 task-specific model을 만드는 것보다 더 좋은 성능을 나타냄
- 더 큰 모형을 사용할수록, 더 많은 dataset을 사용할수록 더 성능이 좋음

## T5

### Text-to-Text Framework

![image](https://user-images.githubusercontent.com/80622859/207849383-26637286-3c69-4bd3-9f41-9b8fb3ab2af8.png)

- NLP task들을 text-to-text 문제로 취급할 수 있음
- 위와 같은 방식은 보통 생성 분야에서 사용
- T5는 분류와 회귀 또한 위와 같은 방식으로 풀려고 함
- 다양한 downstream task에 동일한 model, object, training procedure, decoding process를 적용 가능

### Original Encoder-Decoder Transformer
- 기본 transformer 구조를 크게 벗어나지 않음
- Transformer의 layer normalization에 사용되는 편향을 제거하고 rescale만 수행
- Absolute positional embedding 대신에 relative positional embedding 사용
- 모형의 층 전체에서 position embedding parameter sharing
