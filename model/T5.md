# Exploring the Limits of Transfer Learning with a Unifed Text-to-Text Transformer

## 목적
- 최근 NLP 연구들은 대량의 unsupervised dataset에 대해 사전훈련된 모형을 목표로 하는 downstream task에 대해 지도 학습을 통한 미세 조정을 하는 전이 학습 방식이 보편화
- 위와 같은 방식이 task-specific model을 만드는 것보다 더 좋은 성능을 나타냄
- 더 큰 모형을 사용할수록, 더 많은 dataset을 사용할수록 더 성능이 좋음

## T5

### Text-to-Text Framework

![image](https://user-images.githubusercontent.com/80622859/207849383-26637286-3c69-4bd3-9f41-9b8fb3ab2af8.png)

- NLP task들을 text-to-text 문제로 취급할 수 있음
- 위와 같은 방식은 보통 생성 분야에서 사용
- T5는 분류와 회귀 또한 위와 같은 방식으로 풀려고 함
- 다양한 downstream task에 동일한 model, object, training procedure, decoding process를 적용 가능

### Original Encoder-Decoder Transformer
- 기본 transformer 구조를 크게 벗어나지 않음
- Transformer의 layer normalization에 사용되는 편향을 제거하고 rescale만 수행
- Absolute positional embedding 대신에 relative positional embedding 사용
- 모형의 층 전체에서 position embedding parameter sharing

### Denoising Corrupted Span

![image](https://user-images.githubusercontent.com/80622859/207850423-97d484f2-7df6-42f0-9a2d-0ad33da74b55.png)

- SpanBERT 논문에서 제안된 기법 사용
- Span을 하나의 [MASK] token으로 
- 성능 향상과 계산 효율

### Why Denoising Span?

![image](https://user-images.githubusercontent.com/80622859/207850572-00168825-1beb-48d4-85d2-42aef9bde1a7.png)


- Prefix language modeling : GPT와 같은 standard language modeling
- BERT-style : BERT에서 사용되는 masked language modeling 방식
- Deshuffling : Sequence를 입력으로 받아 순서를 섞은 후 원래 sequence를 목표로 해서 복구하는 방식

### C4 Dataset
- Colossal Clean Crawled Corpus
1. 마침표, 느낌표, 물음표, 온점으로 끝나는 줄만 
2. 문장 5개 미만은 제거, 3단어 이상으로 이루어진 줄만
3. 비속어 담은 문장 제거
4. JavaScript 단어 들어간 줄 제거
5. 무의미한 문장 제거
6. 중괄호 포함한 문장 제거
7. 중복 문장 제거(3번 이상 나올 시 하나만 남김
- langdetect를 통해 영어로만 이루어진 corpus 구성

### Multi-task pre-training
- 하나의 비지도 작업에 대해서 사전학습을 진행한 후 미세조정하는 대신에 여러 종류의 작업에 대해서 한 번에 학습을 진행하는 것
- Multi-task-pre-training + fine-tuning

## Final T5 Model & Takeaways
- Text-to-Text Framework
- Original Transformer Architecture
- Denoising Span Objective
- C4 Dataset
